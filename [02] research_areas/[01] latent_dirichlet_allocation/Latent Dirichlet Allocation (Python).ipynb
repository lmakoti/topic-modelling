{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e59eee0-3498-4807-91ca-b472c946a96d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Latent Dirichlet Allocation (LDA) with Python\n",
    "**Author:** [Jordan Barber](http://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html) <br>\n",
    "**Modified by:** [Lehlohonolo Makoti](https://github.com/lmakoti) <br>\n",
    "**Notebook/Application:** [Databricks Notebooks](https://docs.databricks.com/en/notebooks/index.html)\n",
    "\n",
    "## What is Latent Dirichlet Allocation\n",
    "**Latent Dirichlet Allocation (LDA)** is a statistical model that is used to classify or categorize large sets of data, like documents, into topics. Imagine you have a bunch of newspaper articles, but you don't know what topics they cover. LDA helps by identifying groups of words (topics) that often appear together in these articles. Each article can contain multiple topics in different proportions. For instance, an article might be 70% about sports, 20% about politics, and 10% about the economy. The `\"latent\"` part means that these topics are hidden (not directly observed) and are inferred from the words in the documents. The `\"Dirichlet\"` part refers to the specific type of statistical distribution used in the model to handle the variability of topics in documents.\n",
    "\n",
    "\n",
    "## Packages Required\n",
    "\n",
    "This walkthrough uses the following Python packages:\n",
    "* [NLTK](https://pypi.org/project/nltk/), a natural language toolkit for Python. A useful package for any natural language processing.<br>\n",
    "For Mac/Unix with pip: `$ sudo pip install -U nltk`\n",
    "* [stop_words](https://pypi.org/project/stop-words/), a Python package containing stop words.<br>\n",
    "For Mac/Unix with pip: `$ sudo pip install stop-words`\n",
    "* [gensim](https://pypi.org/project/gensim/), a topic modeling package containing our LDA model.<br>\n",
    "For Mac/Unix with pip: `$ sudo pip install gensim`\n",
    "\n",
    "**Databricks:** `%pip install <package_name>`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8970fede-a5d8-4390-8ef0-ab13f3776842",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Inline library installation\n",
    "%pip install nltk\n",
    "%pip install stop-words\n",
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c25b5e3-072a-4be8-b2c3-dae77bd2e760",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1. Importing the Documents (API/Scrapping/Corpora etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "354b225c-6f8c-4b02-92f5-2f8427a00795",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Import the documents - Any sources are acceptable\n",
    "doc_a = \"Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.\"\n",
    "doc_b = \"My mother spends a lot of time driving my brother around to baseball practice.\"\n",
    "doc_c = \"Some health experts suggest that driving may cause increased tension and blood pressure.\"\n",
    "doc_d = \"I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\"\n",
    "doc_e = \"Health professionals say that brocolli is good for your health.\"\n",
    "\n",
    "# compile sample documents into a list\n",
    "doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5570c339-ba12-4da7-8849-f588c3ca520d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2. Text Preprocessing (Cleaning the Documents)\n",
    "Preprocessing text is essential for Latent Dirichlet Allocation (LDA) and other natural language processing tasks for several reasons:\n",
    "\n",
    "* **Tokenisation:** This involves breaking down text into individual words or terms (tokens). It's a fundamental step for the LDA model to understand and process the text data.\n",
    "\n",
    "* **Reducing Dimensionality (Stopwords removal):** Text data can be very high-dimensional due to the vast number of unique words. Preprocessing steps like removing common stopwords (e.g., \"the\", \"is\", \"and\") reduces the dimensionality, making the LDA model more efficient and effective.\n",
    "\n",
    "* **Stemming/Lemmatization:** These processes reduce words to their root form. For example, \"running\", \"ran\", and \"runs\" would all be reduced to \"run\". This helps in consolidating the variations of a word into a single term, improving the model's ability to identify relevant topics.\n",
    "\n",
    "* **Removing Noise:** Text data often contains elements like special characters, punctuation, and numbers that may not be relevant for topic modeling. Removing these helps focus on meaningful words.\n",
    "\n",
    "* **Standardisation:** Converting all text to a standard format, typically lower case, ensures that the algorithm treats words like \"Apple\" and \"apple\" as the same word.\n",
    "\n",
    "* **Removing Rare Words:** Words that appear very infrequently may not be useful in identifying common themes and topics. Removing these can help improve the model's focus and performance.\n",
    "\n",
    "By preprocessing text, you essentially clean and refine the data, making it more suitable for the LDA model to analyze and draw meaningful conclusions about the underlying topics in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2d7f1e7-ad2c-42c3-a788-c6ba03f168ad",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2.1 Tokenisation/Lowercasing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc004d12-b48b-4f3e-865f-98958079692c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# standardisation/lowering the case\n",
    "raw = doc_a.lower()\n",
    "tokens = tokenizer.tokenize(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "722ae352-6114-44e8-8b1e-9516f431a2fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# handling special text and characters\n",
    "# Remove special characters\n",
    "doc_a = re.sub(r'[^\\w\\s]', '', doc_a)\n",
    "\n",
    "# Remove HTML/XML tags\n",
    "doc_a = re.sub(r'<[^>]+>', '', doc_a)\n",
    "\n",
    "# Remove phone numbers (US format)\n",
    "doc_a = re.sub(r'\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b', '', doc_a)\n",
    "\n",
    "# Example of removing simple JSON/XML structures (not recommended for complex structures)\n",
    "# doc_a = re.sub(r'\\{.*?\\}', '', doc_a) # for simple JSON\n",
    "# doc_a = re.sub(r'<.*?>', '', doc_a) # for simple XML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbdf7253-992b-4956-bb43-aeeaae45371f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2.2 Stopwords Removal/Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82e70b5a-e8b2-42a9-80d0-7a82a4cac918",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f66c0746-b0d1-4a84-94cb-ebb5113cb08c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2.3 Stemming/Lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8854c096-0f2c-460a-8fc0-cc024c21d3ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Stemmer instantiation\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dae18c9c-6f33-4907-aacf-e5e80adb9f40",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Lemmatiser instantiation\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "\n",
    "# Initialize wordnet lemmatizer\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c043121d-0a0f-44ae-adb6-6b8673f96843",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 3. Constructing a Document-Term Matrix\n",
    "The result of our cleaning stage is texts, a tokenized, stopped and stemmed list of words from a single document. Let’s fast forward and imagine that we looped through all our documents and appended each one to texts. So now texts is a list of lists, one list for each of our original documents.\n",
    "\n",
    "To generate an LDA model, we need to understand how frequently each term occurs within each document. To do that, we need to construct a document-term matrix with a package called gensim:\n",
    "```python\n",
    "from gensim import corpora, models\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "```\n",
    "\n",
    "The Dictionary() function traverses texts, assigning a unique integer id to each unique token while also collecting word counts and relevant statistics. To see each token’s unique integer id, try print(dictionary.token2id).\n",
    "\n",
    "Next, our dictionary must be converted into a bag-of-words:\n",
    "\n",
    "`corpus = [dictionary.doc2bow(text) for text in texts]`\n",
    "\n",
    "The doc2bow() function converts dictionary into a bag-of-words. The result, corpus, is a list of vectors equal to the number of documents. In each document vector is a series of tuples. As an example, print(corpus[0]) results in the following:\n",
    "\n",
    "```python\n",
    "print(corpus[0])\n",
    "[(0, 2), (1, 1), (2, 2), (3, 2), (4, 1), (5, 1)]\n",
    "```\n",
    "\n",
    "This list of tuples represents our first document, doc_a. The tuples are (term ID, term frequency) pairs, so if print(dictionary.token2id) says brocolli’s id is 0, then the first tuple indicates that brocolli appeared twice in doc_a. doc2bow() only includes terms that actually occur: terms that do not occur in a document will not appear in that document’s vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b05c54ff-48ed-4aaf-9457-087f3839123e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 4. Applying the LDA model\n",
    "**Theory:** https://github.com/lmakoti/topic-modelling\n",
    "\n",
    "`corpus` is a document-term matrix and now we’re ready to generate an LDA model:\n",
    "\n",
    "```python\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=3, id2word = dictionary, passes=20)\n",
    "```\n",
    "The LDAModel class is described in detail in the [gensim documentation](https://radimrehurek.com/gensim/models/ldamodel.html). Parameters used in our example:\n",
    "* `num_topics`: (required) An LDA model requires the user to determine how many topics should be generated. Our document set is small, so we’re only asking for three topics.\n",
    "* `id2word`: (required) The LdaModel class requires our previous dictionary to map ids to strings.\n",
    "* `passes`: (optional) The number of laps the model will take through corpus. The greater the number of passes, the more accurate the model will be. A lot of passes can be slow on a very large corpus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06037479-0665-46b3-a4de-418711c51e28",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 5. Examining the Results\n",
    "\n",
    "Our LDA model is now stored as ldamodel. We can review our topics with the print_topic and print_topics methods:\n",
    "\n",
    "```python\n",
    "print(ldamodel.print_topics(num_topics=3, num_words=3))\n",
    "['0.141*health + 0.080*brocolli + 0.080*good', '0.060*eat + 0.060*drive + 0.060*brother', '0.059*pressur + 0.059*mother + 0.059*brother']\n",
    "```\n",
    "\n",
    "What does this mean? Each generated topic is separated by a comma. Within each topic are the three most probable words to appear in that topic. Even though our document set is small the model is reasonable. Some things to think about: - health, brocolli and good make sense together. - The second topic is confusing. If we revisit the original documents, we see that drive has multiple meanings: driving a car and driving oneself to improve. This is something to note in our results. - The third topic includes mother and brother, which is reasonable.\n",
    "\n",
    "Adjusting the model’s number of topics and passes is important to getting a good result. Two topics seems like a better fit for our documents:\n",
    "\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word = dictionary, passes=20)\n",
    "\n",
    "```python\n",
    "print(ldamodel.print_topics(num_topics=2, num_words=4))\n",
    "['0.054*pressur + 0.054*drive + 0.054*brother + 0.054*mother', '0.070*brocolli + 0.070*good + 0.070*health + 0.050*eat']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9496901b-79d3-4dec-9f07-e414b6c4c9cf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 6. Putting Everything Together\n",
    "This explanation is a little lengthy, but useful for understanding the model we worked so hard to generate.\n",
    "\n",
    "LDA assumes documents are produced from a mixture of topics. Those topics then generate words based on their probability distribution, like the ones in our walkthrough model. In other words, LDA assumes a document is made from the following steps:\n",
    "\n",
    "* Determine the number of words in a document. Let’s say our document has 6 words.\n",
    "* Determine the mixture of topics in that document. For example, the document might contain 1/2 the topic “health” and 1/2 the topic “vegetables.”\n",
    "* Using each topic’s multinomial distribution, output words to fill the document’s word slots. In our example, the “health” topic is 1/2 our document, or 3 words. The “health” topic might have the word “diet” at 20% probability or “exercise” at 15%, so it will fill the document word slots based on those probabilities.\n",
    "\n",
    "Given this assumption of how documents are created, LDA backtracks and tries to figure out what topics would create those documents in the first place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aadfdef3-41c4-4ebc-9c1f-e2c35875e147",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "    \n",
    "# create sample documents\n",
    "doc_a = \"Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.\"\n",
    "doc_b = \"My mother spends a lot of time driving my brother around to baseball practice.\"\n",
    "doc_c = \"Some health experts suggest that driving may cause increased tension and blood pressure.\"\n",
    "doc_d = \"I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\"\n",
    "doc_e = \"Health professionals say that brocolli is good for your health.\" \n",
    "\n",
    "# compile sample documents into a list\n",
    "doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e]\n",
    "\n",
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "\n",
    "# loop through document list\n",
    "for i in doc_set:\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)\n",
    "\n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "    \n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=4, id2word = dictionary, passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7246d7e-31f5-43fb-988a-513e7343de6a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(ldamodel.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afdfe036-471f-4af6-8029-4eb68ae556ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(ldamodel.print_topics(num_topics=3, num_words=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0f0e57e-a27d-4d41-85ee-d63143edce3e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(ldamodel.print_topics(num_topics=2, num_words=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8680380-9c10-4a64-a2b8-ecaef8a765fb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 7. Visualise the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7561a61a-918e-4397-8b7a-76f5efa5cc91",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Wordcloud of Top N words in each topic\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "cloud = WordCloud(stopwords=stopped_tokens,\n",
    "                  background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=10,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "topics = ldamodel.show_topics(formatted=False)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10,10), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Latent Dirichlet Allocation (Python)",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
