{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e59eee0-3498-4807-91ca-b472c946a96d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Non-negative Matrix Factorisation (NMF) with Python\n",
    "**Author:** [Vijay Choubey](https://medium.com/voice-tech-podcast/topic-modelling-using-nmf-2f510d962b6e) <br>\n",
    "**Modified by:** [Lehlohonolo Makoti](https://github.com/lmakoti) <br>\n",
    "**Notebook/Application:** [Databricks Notebooks](https://docs.databricks.com/en/notebooks/index.html)\n",
    "\n",
    "## What is Non-Negative Matrix Factorisation\n",
    "**Non-Negative Matrix Factorisation** is a statistical method to reduce the dimensions of the input corpora. It uses factor analysis method to provide comparatively less weightage to the words with less coherence.\n",
    "\n",
    "**Translation:** Non-Negative Matrix Factorisation (NMF) is a technique used to simplify large sets of data. It works by breaking down the data into smaller parts and focuses more on the parts that make the most sense together, while giving less importance to less relevant parts. This method helps in understanding and analysing large and complex data more easily.\n",
    "\n",
    "**For the math behind NMF visit:** https://medium.com/voice-tech-podcast/topic-modelling-using-nmf-2f510d962b6e\n",
    "\n",
    "The techniques discussed include:\n",
    "- Generalized Kullbackâ€“Leibler divergence\n",
    "- Frobenius Form (Euclidean Norm)\n",
    "\n",
    "Optimisation is required to achieve high accuracy in finding relation between the topics, this is pre-packaged in `scikit-learn`.\n",
    "\n",
    "## Packages Required\n",
    "\n",
    "This walkthrough uses the following Python packages:\n",
    "* [Numpy](https://pypi.org/project/numpy/), is the fundamental package for scientific computing with Python.<br>\n",
    "For Mac/Unix with pip: `$ sudo pip install -U numpy`\n",
    "* [Scikit-Learn](pip install scikit-learn), is a Python module for machine learning built on top of SciPy and is distributed under the 3-Clause BSD license.<br>\n",
    "For Mac/Unix with pip: `$ sudo pip install scikit-learn`\n",
    "\n",
    "**Databricks:** `%pip install <package_name>`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8970fede-a5d8-4390-8ef0-ab13f3776842",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importing Necessary packages\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c25b5e3-072a-4be8-b2c3-dae77bd2e760",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1. Importing the Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "993b336c-4a8b-4d2d-b153-bcd51381c4b1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 1.1 Loading text from the Scikit-Learn 20 NewsGroups dataset\n",
    "The [20 newsgroups dataset](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html) comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon a messages posted before and after a specific date.\n",
    "\n",
    "This dataset is a collection newsgroup documents. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering.\n",
    "\n",
    "**Alternative Source:** [Kaggle 20 Newsgroups](https://www.kaggle.com/datasets/crawford/20-newsgroups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "507f4ea9-1e5b-48b9-a552-3df20a0bc045",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importing Data\n",
    "text_data= fetch_20newsgroups(remove=('headers', 'footers', 'quotes')).data\n",
    "text_data[:2] # importing the first three articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5570c339-ba12-4da7-8849-f588c3ca520d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2. Text Preprocessing\n",
    "Preprocessing text is essential for Latent Dirichlet Allocation (LDA) and other natural language processing tasks for several reasons:\n",
    "\n",
    "* **Tokenisation:** This involves breaking down text into individual words or terms (tokens). It's a fundamental step for the LDA model to understand and process the text data.\n",
    "\n",
    "* **Reducing Dimensionality (Stopwords removal):** Text data can be very high-dimensional due to the vast number of unique words. Preprocessing steps like removing common stopwords (e.g., \"the\", \"is\", \"and\") reduces the dimensionality, making the LDA model more efficient and effective.\n",
    "\n",
    "* **Stemming/Lemmatization:** These processes reduce words to their root form. For example, \"running\", \"ran\", and \"runs\" would all be reduced to \"run\". This helps in consolidating the variations of a word into a single term, improving the model's ability to identify relevant topics.\n",
    "\n",
    "* **Removing Noise:** Text data often contains elements like special characters, punctuation, and numbers that may not be relevant for topic modeling. Removing these helps focus on meaningful words.\n",
    "\n",
    "* **Standardisation:** Converting all text to a standard format, typically lower case, ensures that the algorithm treats words like \"Apple\" and \"apple\" as the same word.\n",
    "\n",
    "* **Removing Rare Words:** Words that appear very infrequently may not be useful in identifying common themes and topics. Removing these can help improve the model's focus and performance.\n",
    "\n",
    "By preprocessing text, you essentially clean and refine the data, making it more suitable for the LDA model to analyze and draw meaningful conclusions about the underlying topics in the text.\n",
    "\n",
    "**NB:** for NMF while not strictly necessary, applying lemmatisation or stemming before NMF can be beneficial. It can improve the efficiency of the matrix factorisation and potentially lead to more coherent and interpretable results, especially when dealing with large and diverse text corpora. The choice between stemming and lemmatisation should be guided by the specific requirements of your analysis and the nature of your text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2d7f1e7-ad2c-42c3-a788-c6ba03f168ad",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2.1 Tokenisation | Lowercasing\n",
    "This `tokenizer` is often used in text processing to break text into words while filtering out punctuation and other non-word characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc004d12-b48b-4f3e-865f-98958079692c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "# standardisation/lowering the case\n",
    "raw = str(text_data).lower() # convert to string because 'list' object has no attribute 'lower'\n",
    "tokens = str(tokenizer.tokenize(raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "722ae352-6114-44e8-8b1e-9516f431a2fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# handling special text and characters\n",
    "# Remove special characters\n",
    "tokens = re.sub(r'[^\\w\\s]', '', tokens)\n",
    "\n",
    "# Remove HTML/XML tags\n",
    "tokens = re.sub(r'<[^>]+>', '', tokens)\n",
    "\n",
    "# Remove phone numbers (US format)\n",
    "tokens = re.sub(r'\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b', '', tokens)\n",
    "\n",
    "# Example of removing simple JSON/XML structures (not recommended for complex structures)\n",
    "tokens = re.sub(r'\\{.*?\\}', '', tokens) # for simple JSON\n",
    "tokens = re.sub(r'<.*?>', '', tokens) # for simple XML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c043121d-0a0f-44ae-adb6-6b8673f96843",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 3. Constructing a Document-Term Matrix\n",
    "Now we convert a collection of text documents into a numeric form that machine learning algorithms can work with, focusing on the importance of words in the documents while filtering out common words and limiting the total number of words to manage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2b1d112-096f-4e38-b93f-4e2df2c8f04b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Converting the given text term-document matrix\n",
    "vectorizer = TfidfVectorizer(max_features=1500, min_df=10, stop_words='english')\n",
    "X = vectorizer.fit_transform(text_data)\n",
    "words = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "print(X)\n",
    "print(\"X = \", words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b05c54ff-48ed-4aaf-9457-087f3839123e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 4. Applying Non-Negative Matrix Factorization\n",
    "**Theory:** https://github.com/lmakoti/topic-modelling\n",
    "\n",
    "For a general case, consider we have an input matrix V of shape m x n. This method factorizes V into two matrices W and H, such that the dimension of W is m x k and that of H is n x k. For our situation, V represent the term document matrix, each row of matrix H is a word embedding and each column of the matrix W represent the weightage of each word get in each sentences ( semantic relation of words with each sentence). You can find a practical application with example below.\n",
    "\n",
    "But the assumption here is that all the entries of W and H is positive given that all the entries of V is positive.\n",
    "<br><br>\n",
    "<p align=\"center\"><img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/0*uz3OkHMgjAH2Yc40.png\"/></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0aa942e-7d22-4ff6-9a4f-dd20357ba8b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=10, solver=\"mu\")\n",
    "W = nmf.fit_transform(X)\n",
    "H = nmf.components_\n",
    "\n",
    "for i, topic in enumerate(H):\n",
    "     print(\"Topic {}: {}\".format(i + 1, \",\".join([str(x) for x in words[topic.argsort()[-10:]]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06037479-0665-46b3-a4de-418711c51e28",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 5. Examining the Results\n",
    "When can we use this approach:\n",
    "- NMF by default produces sparse representations. This mean that most of the entries are close to zero and only very few parameters have significant values. This can be used when we strictly require fewer topics.\n",
    "- NMF produces more coherent topics compared to LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f4f25c0-f6dc-4a36-96f9-f6c8616a0e28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# W matrix is given below.\n",
    "print(W[:10,:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23de2f5f-39b9-42c2-baa2-86fc6bd0132e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# H matrix can be printed as shown below.\n",
    "print(H[:10,:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9496901b-79d3-4dec-9f07-e414b6c4c9cf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 6. Putting Everything Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aadfdef3-41c4-4ebc-9c1f-e2c35875e147",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importing Data\n",
    "text_data= fetch_20newsgroups(remove=('headers', 'footers', 'quotes')).data\n",
    "text_data[:2] # importing the first three articles\n",
    "\n",
    "# Importing Data\n",
    "text_data= fetch_20newsgroups(remove=('headers', 'footers', 'quotes')).data\n",
    "text_data[:3]\n",
    "\n",
    "# converting the given text term-document matrix\n",
    "vectorizer = TfidfVectorizer(max_features=1500, min_df=10, stop_words='english')\n",
    "X = vectorizer.fit_transform(text_data)\n",
    "words = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "# print(X)\n",
    "# print(\"X = \", words)\n",
    "\n",
    "# Applying Non-Negative Matrix Factorization\n",
    "nmf = NMF(n_components=10, solver=\"mu\")\n",
    "W = nmf.fit_transform(X)\n",
    "H = nmf.components_\n",
    "\n",
    "for i, topic in enumerate(H):\n",
    "     print(\"Topic {}: {}\".format(i + 1, \",\".join([str(x) for x in words[topic.argsort()[-10:]]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7246d7e-31f5-43fb-988a-513e7343de6a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(W[:10,:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afdfe036-471f-4af6-8029-4eb68ae556ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(H[:10,:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8680380-9c10-4a64-a2b8-ecaef8a765fb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 7. Visualise the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7561a61a-918e-4397-8b7a-76f5efa5cc91",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'H' is your topic matrix and 'words' is your array of words\n",
    "for i, topic in enumerate(H):\n",
    "    top_words_indices = topic.argsort()[-10:]  # Indices of top 10 words in this topic\n",
    "    top_words = [words[j] for j in top_words_indices]  # Top 10 words\n",
    "    top_words_weights = [topic[j] for j in top_words_indices]  # Weights of top 10 words\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.barh(range(10), top_words_weights, align='center')\n",
    "    plt.yticks(range(10), top_words)\n",
    "    plt.gca().invert_yaxis()  # Invert y-axis to have the highest value on top\n",
    "    plt.title(f'Topic {i + 1}')\n",
    "    plt.xlabel('Weights')\n",
    "    plt.ylabel('Words')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2ec312b-b2ed-40a6-8453-f7dd91701b36",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Assuming 'H' is your topic matrix and 'words' is your array of words\n",
    "for i, topic in enumerate(H):\n",
    "    # Generate a dictionary of word frequencies for this topic\n",
    "    word_freq = {words[j]: topic[j] for j in topic.argsort()[-10:]}\n",
    "\n",
    "    # Normalise frequencies for better visualisation\n",
    "    max_freq = max(word_freq.values())\n",
    "    word_freq = {word: freq / max_freq for word, freq in word_freq.items()}\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.title(f'Topic {i + 1}')\n",
    "    plt.axis('off')\n",
    "    for word, freq in word_freq.items():\n",
    "        plt.text(random.uniform(0, 1), random.uniform(0, 1), word, \n",
    "                 ha='center', va='center',\n",
    "                 fontsize=freq * 40)  # Adjust font size based on frequency\n",
    "\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Non-negative Matrix Factorisation",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
